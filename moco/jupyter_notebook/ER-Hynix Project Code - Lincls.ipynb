{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-Hynix Project Code - Lincls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SK-Hynix 프로젝트에서 진행한 연구의 실험 중, 학습된 encoder를 linear classification 문제로 transfer learning을 하는 Jupyter notebook 파일입니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  1. 'Mixup 기법으로 얻은 representation을 contrastive task에 사용하는 것'이 본 연구의 핵심 아이디어입니다.\n",
    "\n",
    "#### 2. 현재는 [MoCo](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf) 논문을 기반으로 아이디어를 구현했습니다.\n",
    "\n",
    "#### 3. 아쉽게도 multi-gpu 실험은 현 jupyter notebook에서는 불가능합니다. 각 함수들의 기능만 봐주시면 될 것 같습니다.\n",
    "\n",
    "#### <span style=\"color:red\"> 4. Lincls(Linear Classification) part의 경우, Pretrain part와 매우 유사하므로 달라진 부분만 표시해놓겠습니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library setting\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os, math, random, time, shutil, builtins, argparse, warnings, json, glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 1] 데이터 불러오기\n",
    "\n",
    "#### CIFAR-10과 CIFAR-100 데이터의 경우, torchvision.datasets library에서 받아옵니다.\n",
    "#### 반면 Tiny-ImageNet 실험의 경우, 현 directory 안에 data/tiny-imagenet-200 이 저장되어 있어야합니다. Tiny-ImageNet-200의 경우 [Tiny-ImageNet](https://tiny-imagenet.herokuapp.com/)에서 다운로드 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10, CIFAR-100 dataset\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "\n",
    "\n",
    "# Tiny-ImageNet dataset\n",
    "EXTENSION = 'JPEG'\n",
    "NUM_IMAGES_PER_CLASS = 500\n",
    "CLASS_LIST_FILE = 'wnids.txt'\n",
    "VAL_ANNOTATION_FILE = 'val_annotations.txt'\n",
    "\n",
    "class TinyImageNet(Dataset):\n",
    "    \"\"\"Tiny ImageNet data set available from `http://cs231n.stanford.edu/tiny-imagenet-200.zip`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    root: string\n",
    "        Root directory including `train`, `test` and `val` subdirectories.\n",
    "    split: string\n",
    "        Indicating which split to return as a data set.\n",
    "        Valid option: [`train`, `test`, `val`]\n",
    "    transform: torchvision.transforms\n",
    "        A (series) of valid transformation(s).\n",
    "    in_memory: bool\n",
    "        Set to True if there is enough memory (about 5G) and want to minimize disk IO overhead.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, in_memory=False, download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.train = train\n",
    "        self.split = 'train' if train else 'val'\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.in_memory = in_memory\n",
    "        self.split_dir = os.path.join(root, self.split)\n",
    "        self.image_paths = sorted(glob.iglob(os.path.join(self.split_dir, '**', '*.%s' % EXTENSION), recursive=True))\n",
    "        self.labels = {}  # fname - label number mapping\n",
    "        self.images = []  # used for in-memory processing\n",
    "\n",
    "        # build class label - number mapping\n",
    "        with open(os.path.join(self.root, CLASS_LIST_FILE), 'r') as fp:\n",
    "            self.label_texts = sorted([text.strip() for text in fp.readlines()])\n",
    "        self.label_text_to_number = {text: i for i, text in enumerate(self.label_texts)}\n",
    "\n",
    "        if self.split == 'train':\n",
    "            for label_text, i in self.label_text_to_number.items():\n",
    "                for cnt in range(NUM_IMAGES_PER_CLASS):\n",
    "                    self.labels['%s_%d.%s' % (label_text, cnt, EXTENSION)] = i\n",
    "        elif self.split == 'val':\n",
    "            with open(os.path.join(self.split_dir, VAL_ANNOTATION_FILE), 'r') as fp:\n",
    "                for line in fp.readlines():\n",
    "                    terms = line.split('\\t')\n",
    "                    file_name, label_text = terms[0], terms[1]\n",
    "                    self.labels[file_name] = self.label_text_to_number[label_text]\n",
    "\n",
    "        # read all images into torch tensor in memory to minimize disk IO overhead\n",
    "        if self.in_memory:\n",
    "            self.images = [self.read_image(path) for path in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.image_paths[index]\n",
    "\n",
    "        if self.in_memory:\n",
    "            img = self.images[index]\n",
    "        else:\n",
    "            img = self.read_image(file_path)\n",
    "\n",
    "        if self.split == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            # file_name = file_path.split('/')[-1]\n",
    "            return img, self.labels[os.path.basename(file_path)]\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = self.split\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "    def read_image(self, path):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        return self.transform(img) if self.transform else img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning에 사용될 데이터셋을 pytorch library의 DataLoader로 옮기는 코드입니다.\n",
    "####  <span style=\"color:red\"> pretrain part와 달리 train_loader와 더불어 모델의 성능을 평가할 valid_loader 또한 정의하였습니다.</span>\n",
    "####  <span style=\"color:red\"> Augmentation은 일반적인 vision task에서 쓰이는 기본적인 기법들만 사용하였습니다. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "DATASETS = {'cifar10': CIFAR10, 'cifar100': CIFAR100, 'tiny-imagenet': TinyImageNet}\n",
    "MEAN = {'cifar10': [0.4914, 0.4822, 0.4465], 'cifar100': [0.5071, 0.4867, 0.4408], 'tiny-imagenet': [0.485, 0.456, 0.406]}\n",
    "STD = {'cifar10': [0.2023, 0.1994, 0.2010], 'cifar100':[0.2675, 0.2565, 0.2761], 'tiny-imagenet': [0.229, 0.224, 0.225]}\n",
    "\n",
    "def data_loader(dataset, data_path, batch_size, num_workers, download=False, distributed=True):\n",
    "    normalize = transforms.Normalize(MEAN[dataset], STD[dataset])\n",
    "    \n",
    "    train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          normalize])\n",
    "        \n",
    "    test_transform=transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       normalize])\n",
    "\n",
    "    train_dataset = DATASETS[dataset](data_path, train=True, download=download, transform=train_transform)\n",
    "    test_dataset = DATASETS[dataset](data_path, train=False, download=download, transform=test_transform)\n",
    "    \n",
    "    # for distributed learning\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if distributed else None\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),\n",
    "        num_workers=num_workers, pin_memory=True, sampler=train_sampler, drop_last=True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    \n",
    "    return train_loader, val_loader, train_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 2] 모델 불러오기\n",
    "\n",
    "#### 이전에 pretraining 때 사용했던 <span style=\"color:red\"> base_encoder 모델의 구조(ex. ResNet)</span>를 들고와야, 'load_state_dict' 함수를 통해 학습된 weight를 불러올 수 있습니다.\n",
    "\n",
    "#### <span style=\"color:red\">이후 학습을 할때는 fc.weight와 fc.bias를 제외한 나머지 parameter는 gradient가 흐르지 않도록 만든채, linear classifier인 fully-connected layer만을 학습할 것입니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet code\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    __constants__ = ['downsample']\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, \n",
    "                 base_width=64, dilation=1, norm_layer=None, num_splits=64, expansion=1, block_type='basic'):\n",
    "        super(Block, self).__init__()\n",
    "        if block_type not in ['basic', 'bottleneck']:\n",
    "            raise ValueError('Block_Type only supports basic and bottleneck')\n",
    "        self.block_type = block_type\n",
    "        self.expansion = expansion\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "            self.split = False\n",
    "        else:\n",
    "            self.split = True\n",
    "            \n",
    "        if block_type == 'basic':\n",
    "            if groups != 1 or base_width != 64:\n",
    "                raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "            if dilation > 1:\n",
    "                raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        \n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both conv3*3 with stride and self.downsample layers downsample the input when stride != 1\n",
    "        if block_type == 'basic':\n",
    "            self.conv1 = conv3x3(inplanes, width, stride)\n",
    "            self.conv2 = conv3x3(width, width)\n",
    "            \n",
    "        if block_type == 'bottleneck':\n",
    "            self.conv1 = conv1x1(inplanes, width)\n",
    "            self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "            self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "            self.bn3 = norm_layer(planes * self.expansion) if not self.split else norm_layer(planes * self.expansion, num_splits)\n",
    "            \n",
    "        self.bn1 = norm_layer(width) if not self.split else norm_layer(width, num_splits)\n",
    "        self.bn2 = norm_layer(width) if not self.split else norm_layer(width, num_splits)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.block_type == 'bottleneck':\n",
    "            out = self.relu(out)\n",
    "            \n",
    "            out = self.conv3(out)\n",
    "            out = self.bn3(out)\n",
    "            \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    BasicBlock_arch = ['resnet10', 'resnet18', 'resnet34']\n",
    "    Bottleneck_arch = ['resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', \n",
    "                      'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "    def __init__(self, arch, repeats, num_classes=100, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, norm_layer=None, num_splits=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.split = False if norm_layer is None else True\n",
    "        self._norm_layer = nn.BatchNorm2d if norm_layer is None else norm_layer\n",
    "        self.num_splits = num_splits\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        if arch in self.BasicBlock_arch:\n",
    "            self.expansion = 1\n",
    "            self.block_type = 'basic'\n",
    "        elif arch in self.Bottleneck_arch:\n",
    "            self.expansion = 4\n",
    "            self.block_type = 'bottleneck'\n",
    "        else:\n",
    "            raise NotImplementedError('%s arch is not supported in ResNet' % arch)\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                                   bias=False)\n",
    "        self.bn1 = self._norm_layer(self.inplanes) if not self.split else self._norm_layer(self.inplanes, self.num_splits)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)     \n",
    "            \n",
    "        planes = [64, 128, 256, 512]\n",
    "        # self.planes attributes is needed to match with EP_module channels\n",
    "        self.planes = [p * self.expansion for p in planes]\n",
    "        strides = [1, 2, 2, 2]\n",
    "        self.block_layers = self._make_layer(planes, repeats, strides)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(planes[-1] * self.expansion, num_classes)\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Block):\n",
    "                    if self.block_type == 'basic':\n",
    "                        nn.init.constant_(m.bn2.weight, 0)\n",
    "                    elif self.block_type == 'bottleneck':\n",
    "                        nn.init.constant_(m.bn3.weight, 0)\n",
    "                        \n",
    "    def _make_layer(self, planes, repeats, strides):\n",
    "        assert len(planes) == len(repeats) == len(strides) == 4, 'Number of Block should be 4'\n",
    "        \n",
    "        block_layers = []\n",
    "        norm_layer = self._norm_layer\n",
    "        for i in  range(4):\n",
    "            plane = planes[i]\n",
    "            repeat = repeats[i]\n",
    "            stride = strides[i]\n",
    "            \n",
    "            downsample = None\n",
    "            if stride != 1 or self.inplanes != plane * self.expansion:\n",
    "                if not self.split:\n",
    "                    downsample = nn.Sequential(\n",
    "                        conv1x1(self.inplanes, plane * self.expansion, stride),\n",
    "                        norm_layer(plane * self.expansion),\n",
    "                    )\n",
    "                else:\n",
    "                    downsample = nn.Sequential(\n",
    "                        conv1x1(self.inplanes, plane * self.expansion, stride),\n",
    "                        norm_layer(plane * self.expansion, self.num_splits),\n",
    "                    )\n",
    "\n",
    "            layers = []\n",
    "            layers.append(Block(self.inplanes, plane, stride, downsample, self.groups,\n",
    "                                self.base_width, self.dilation, norm_layer, self.num_splits,\n",
    "                                self.expansion, self.block_type))\n",
    "            self.inplanes = plane * self.expansion\n",
    "            for _ in range(1, repeat):\n",
    "                layers.append(Block(self.inplanes, plane, groups=self.groups,\n",
    "                                    base_width=self.base_width, dilation=self.dilation,\n",
    "                                    norm_layer=norm_layer, num_splits=self.num_splits,\n",
    "                                    expansion=self.expansion, block_type=self.block_type))\n",
    "            block_layers.append(nn.Sequential(*layers))\n",
    "        \n",
    "        return nn.Sequential(*block_layers)\n",
    "    \n",
    "    def conv_stem(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def pool_linear(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.block_layers(x)\n",
    "        x = self.pool_linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def _resnet(arch, repeats, **kwargs):\n",
    "    model = ResNet(arch, repeats, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet10(**kwargs):\n",
    "    return _resnet('resnet10', [1, 1, 1, 1], **kwargs)    \n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return _resnet('resnet18', [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return _resnet('resnet34', [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return _resnet('resnet50', [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet101( **kwargs):\n",
    "    return _resnet('resnet101', [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    return _resnet('resnet152', [3, 8, 36, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet50_32x4d(**kwargs):\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnet50_32x4d', [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet101_32x8d(**kwargs):\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnet101_32x8d', [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(**kwargs):\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(**kwargs):\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', [3, 4, 23, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have other architecture, type into ARCHITECTURE dict\n",
    "ARCHITECTURE = {'resnet10': resnet10, 'resnet18': resnet18, 'resnet34': resnet34, 'resnet50': resnet50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MoCo 논문에서 multi-gpu 상황에서 성능 향상을 위해 Batch Shuffling이란 technique을 사용했습니다.\n",
    "#### 하지만 Single-gpu에서는 shuffling 적용이 불가해, encoder에서 사용하는 BN을 다음의 SplitBatchNorm으로 바꿀 것을 권장하였습니다.\n",
    "#### <span style=\"color:red\"> pretrain 때 사용했던 BN으로 맞춰서 설정해야합니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SplitBatchNorm: Same effect with Batch Shuffling in MoCo\n",
    "class SplitBatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, num_splits, **kw):\n",
    "        super().__init__(num_features, **kw)\n",
    "        self.num_splits = num_splits\n",
    "        \n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            running_mean_split = self.running_mean.repeat(self.num_splits)\n",
    "            running_var_split = self.running_var.repeat(self.num_splits)\n",
    "            outcome = nn.functional.batch_norm(\n",
    "                input.view(-1, C * self.num_splits, H, W), running_mean_split, running_var_split, \n",
    "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "                True, self.momentum, self.eps).view(N, C, H, W)\n",
    "            self.running_mean.data.copy_(running_mean_split.view(self.num_splits, C).mean(dim=0))\n",
    "            self.running_var.data.copy_(running_var_split.view(self.num_splits, C).mean(dim=0))\n",
    "            return outcome\n",
    "        else:\n",
    "            return nn.functional.batch_norm(\n",
    "                input, self.running_mean, self.running_var, \n",
    "                self.weight, self.bias, False, self.momentum, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 3] 학습 함수 구현하기\n",
    "\n",
    "#### 먼저 학습 과정에서 사용될 seed 고정, learning_rate 조절, checkpoint 저장 등의 utils 함수들을 구현하였습니다. \n",
    "#### <span style=\"color:red\">pretrain된 모델과 구조가 같은 encoder 모델이 사용되고 있는지 확인하는 'sanity_check'라는 함수 또한 구현하였습니다.</span>\n",
    "#### 그 다음 lincls의 한 epoch 단위의 학습 함수를 구현하였습니다.\n",
    "#### 또한 val_loader에 대한 평가 정확도를 위해, <span style=\"color:red\">'validate'라는 평가 함수를 구현하였습니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def fix_seed(seed):\n",
    "    # fix seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "    warnings.warn('You have chosen to seed training. '\n",
    "                  'This will turn on the CUDNN deterministic setting, '\n",
    "                  'which can slow down your training considerably! '\n",
    "                  'You may see unexpected behavior when restarting '\n",
    "                  'from checkpoints.')\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='test'):\n",
    "    filename = os.path.join('./results/lincls', filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, './results/lincls/model_best.pth.tar')\n",
    "    \n",
    "    \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, cos, num_epochs, schedule):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    lr = lr\n",
    "    if cos:  # cosine lr schedule\n",
    "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / num_epochs))\n",
    "    else:  # stepwise lr schedule\n",
    "        for milestone in schedule:\n",
    "            lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "    \n",
    "    \n",
    "def update_json(exp_name, part='lincls', acc=[0,0], path='./results/results.json'):\n",
    "    acc = [round(acc[0], 3), round(acc[1], 3)]\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        result_dict = json.load(f)\n",
    "    \n",
    "        if exp_name not in result_dict.keys():\n",
    "            result_dict[exp_name] = dict()\n",
    "\n",
    "        result_dict[exp_name][part] = acc\n",
    "    \n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(result_dict, f)\n",
    "        \n",
    "    print('results updated to %s' % path)\n",
    "    \n",
    "\n",
    "def sanity_check(state_dict, pretrained_weights):\n",
    "    \"\"\"\n",
    "    Linear classifier should not change any weights other than the linear layer.\n",
    "    This sanity check asserts nothing wrong happens (e.g., BN stats updated).\n",
    "    \"\"\"\n",
    "    print(\"=> loading '{}' for sanity check\".format(pretrained_weights))\n",
    "    checkpoint = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "    state_dict_pre = checkpoint['state_dict']\n",
    "\n",
    "    for k in list(state_dict.keys()):\n",
    "        # only ignore fc layer\n",
    "        if 'fc.weight' in k or 'fc.bias' in k:\n",
    "            continue\n",
    "\n",
    "        # name in pretrained model\n",
    "        k_pre = 'module.encoder_q.' + k[len('module.'):] if k.startswith('module.') else 'encoder_q.' + k\n",
    "\n",
    "        assert ((state_dict[k].cpu() == state_dict_pre[k_pre]).all()), \\\n",
    "            '{} is changed in linear classifier training.'.format(k)\n",
    "\n",
    "    print(\"=> sanity check passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, criterion, epoch, print_freq, gpu):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    \"\"\"\n",
    "    Switch to eval mode:\n",
    "    Under the protocol of linear classification on frozen features/models,\n",
    "    it is not legitimate to change any part of the pre-trained model.\n",
    "    BatchNorm in train mode may revise running mean/std (even if it receives\n",
    "    no gradient), which are part of the model parameters too.\n",
    "    \"\"\"\n",
    "    # switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if gpu is not None:\n",
    "            images = images.cuda(gpu, non_blocking=True)\n",
    "        target = target.cuda(gpu, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, optimizer, criterion, print_freq, gpu):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            if gpu is not None:\n",
    "                images = images.cuda(gpu, non_blocking=True)\n",
    "            target = target.cuda(gpu, non_blocking=True)\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 4] 분산학습 환경 설정하기\n",
    "\n",
    "#### 이전 Section들에서 정의했던 함수와 class들을 이용해, 전체적인 main_worker 함수를 구현하였습니다. \n",
    "#### pytorch에서 제공하는 분산환경 구축 코드를 참조하여 'main' 함수를 구현였습니다. 'func' argument로는 'main_worker' function을 넣으면 됩니다. \n",
    "#### 참고로, 분산환경을 사용할 때는 데이터를 불러오는 과정에서 DistributedSampler를 사용해야합니다. (Section 1 참고)\n",
    "\n",
    "#### Pretrain 과 달리 linear classifier 만 학습하면 되므로, 손실함수는 일반적인 <span style=\"color:red\">nn.CrossEntropyLoss</span> 를 사용하면 됩니다.\n",
    "#### 또한 optimizer는 오직 <span style=\"color:red\">model의 fully-connected layer parameter만을 업데이트합니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(gpu, ngpus_per_node, exp_name, distributed_kwargs, arch, arch_kwargs, train_kwargs, data_kwargs):\n",
    "    best_acc1 = 0\n",
    "    gpu = gpu\n",
    "\n",
    "    # suppress printing if not master\n",
    "    if distributed_kwargs['multiprocessing_distributed'] and gpu != 0:\n",
    "        def print_pass(*args):\n",
    "            pass\n",
    "        builtins.print = print_pass\n",
    "\n",
    "    if gpu is not None:\n",
    "        print(gpu)\n",
    "        print(\"Use GPU: {} for training\".format(gpu))\n",
    "\n",
    "    if distributed_kwargs['distributed']:\n",
    "        if distributed_kwargs['dist_url'] == \"env://\" and distributed_kwargs['rank'] == -1:\n",
    "            rank = int(os.environ[\"RANK\"])\n",
    "        if distributed_kwargs['multiprocessing_distributed']:\n",
    "            # For multiprocessing distributed training, rank needs to be the\n",
    "            # global rank among all the processes\n",
    "            rank = rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend=distributed_kwargs['dist_backend'], \n",
    "                                init_method=distributed_kwargs['dist_url'],\n",
    "                                world_size=distributed_kwargs['world_size'],\n",
    "                                rank=rank)\n",
    "    # create model\n",
    "    print(\"=> creating model '{}'\".format(arch))\n",
    "    \n",
    "    arch_kwargs['norm_layer'] = SplitBatchNorm if arch_kwargs['single_gpu'] else None\n",
    "    del arch_kwargs['single_gpu']\n",
    "    model = ARCHITECTURE[arch](**arch_kwargs)\n",
    "    print(model)\n",
    "    \n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    # init the fc layer\n",
    "    model.fc.weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.fc.bias.data.zero_()\n",
    "    \n",
    "    # load from pre-trained, before DistributedDataParallel constructor\n",
    "    if train_kwargs['pretrained']:\n",
    "        if os.path.isfile(train_kwargs['pretrained']):\n",
    "            print(\"=> loading checkpoint '{}'\".format(train_kwargs['pretrained']))\n",
    "            checkpoint = torch.load(train_kwargs['pretrained'], map_location=\"cpu\")\n",
    "            #print(checkpoint['state_dict'].keys())\n",
    "            # rename pre-trained keys\n",
    "            state_dict = checkpoint['state_dict']\n",
    "            if distributed_kwargs['distributed']:\n",
    "                for k in list(state_dict.keys()):\n",
    "                    # retain only encoder_q up to before the embedding layer\n",
    "                    if k.startswith('module.encoder_q') and not k.startswith('module.encoder_q.fc'):\n",
    "                        # remove prefix\n",
    "                        state_dict[k[len(\"module.encoder_q.\"):]] = state_dict[k]\n",
    "                    # delete renamed or unused k\n",
    "                    del state_dict[k]\n",
    "            else:\n",
    "                for k in list(state_dict.keys()):\n",
    "                    # retain only encoder_q up to before the embedding layer\n",
    "                    if k.startswith('encoder_q') and not k.startswith('encoder_q.fc'):\n",
    "                        # remove prefix\n",
    "                        state_dict[k[len(\"encoder_q.\"):]] = state_dict[k]\n",
    "                    # delete renamed or unused k\n",
    "                    del state_dict[k]\n",
    "\n",
    "            msg = model.load_state_dict(state_dict, strict=False)\n",
    "            #print(state_dict.keys())\n",
    "            #print(msg.missing_keys)\n",
    "            assert set(msg.missing_keys) == {\"fc.weight\", \"fc.bias\"}\n",
    "\n",
    "            print(\"=> loaded pre-trained model '{}'\".format(train_kwargs['pretrained']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(train_kwargs['pretrained']))\n",
    "            \n",
    "    if distributed_kwargs['distributed']:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if gpu is not None:\n",
    "            torch.cuda.set_device(gpu)\n",
    "            model.cuda(gpu)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            data_kwargs['batch_size'] = int(data_kwargs['batch_size'] / ngpus_per_node)\n",
    "            data_kwargs['num_workers'] = int((data_kwargs['num_workers'] + ngpus_per_node - 1) / ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif gpu is not None:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        model = model.cuda(gpu)\n",
    "        \n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "\n",
    "    # optimize only the linear classifier\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    assert len(parameters) == 2  # fc.weight, fc.bias\n",
    "    optimizer = torch.optim.SGD(parameters, **train_kwargs['opt_kwargs'])\n",
    "\n",
    "    # get train_loader\n",
    "    train_loader, val_loader, train_sampler = data_loader(**data_kwargs)\n",
    "    \n",
    "    for epoch in range(train_kwargs['num_epochs']):\n",
    "        if distributed_kwargs['distributed']:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        adjust_learning_rate(optimizer, epoch, train_kwargs['opt_kwargs']['lr'], train_kwargs['cos'], train_kwargs['num_epochs'], train_kwargs['schedule'])\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, optimizer, criterion, epoch+1, train_kwargs['print_freq'], gpu)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        acc1, acc5 = validate(train_loader, model, optimizer, criterion, train_kwargs['print_freq'], gpu)\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)        \n",
    "\n",
    "    # always saves at the end of training    \n",
    "    else:\n",
    "        if not distributed_kwargs['multiprocessing_distributed'] \\\n",
    "        or (distributed_kwargs['multiprocessing_distributed'] and rank % ngpus_per_node == 0):\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch+1,\n",
    "                'arch': arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best=False, filename='{}.pth.tar'.format(exp_name))\n",
    "            if epoch == 0:\n",
    "                sanity_check(model.state_dict(), train_kwargs['pretrained'])\n",
    "                \n",
    "        update_json(exp_name, 'lincls', [best_acc1.item(), acc5.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(func, exp_name, distributed_kwargs, arch, arch_kwargs, train_kwargs, data_kwargs):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([str(gpu) for gpu in distributed_kwargs['gpu']])\n",
    "    if len(distributed_kwargs['gpu']) > 1:\n",
    "        distributed_kwargs['gpu'] = None\n",
    "    else:\n",
    "        distributed_kwargs['gpu'] = [0]\n",
    "\n",
    "    if distributed_kwargs['gpu'] is not None:\n",
    "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                      'disable data parallelism.')\n",
    "\n",
    "    if distributed_kwargs['dist_url'] == \"env://\" and distributed_kwargs['world_size'] == -1:\n",
    "        distributed_kwargs['world_size'] = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    distributed_kwargs['distributed'] = distributed_kwargs['world_size'] > 1 or distributed_kwargs['multiprocessing_distributed']\n",
    "    distributed_kwargs['ngpus_per_node'] = torch.cuda.device_count()\n",
    "    \n",
    "    data_kwargs['distributed'] = distributed_kwargs['distributed']\n",
    "    \n",
    "    if distributed_kwargs['multiprocessing_distributed']:\n",
    "        # Since we have ngpus_per_node processes per node, the total world_size\n",
    "        # needs to be adjusted accordingly\n",
    "        distributed_kwargs['world_size'] *= distributed_kwargs['ngpus_per_node']\n",
    "        # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
    "        # main_worker process function\n",
    "        mp.spawn(func, nprocs=distributed_kwargs['ngpus_per_node'], args=(distributed_kwargs['ngpus_per_node'], exp_name, distributed_kwargs, arch, arch_kwargs, train_kwargs, data_kwargs))\n",
    "    else:\n",
    "        # Simply call main_worker function\n",
    "        func(distributed_kwargs['gpu'][0], distributed_kwargs['ngpus_per_node'], exp_name, distributed_kwargs, arch, arch_kwargs, train_kwargs, data_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 5] Lincls 학습하기\n",
    "\n",
    "#### Pretrained encoder 모델을 이용해, 라벨이 있는 Tiny-ImageNet 데이터를 학습해볼 것입니다.\n",
    "### <span style=\"color:red\">원하는 실험 셋팅에 필요한 argument를 정의하시면 됩니다.</span>\n",
    "- 아래 세팅은 single-gpu 상황에서의 Tiny-ImageNet 데이터셋 실험입니다.\n",
    "- 다른 데이터셋을 원하시면 dataset argument만 수정하시면 됩니다.\n",
    "- <span style=\"color:red\">하지만, jupyter notebook에서는 multi-gpu를 위한 torch.multiprocessing.spawn을 사용할 수 없어 여기서는 불가능합니다.<span style=\"color:red\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories\n",
    "!mkdir -p './results/lincls'\n",
    "\n",
    "# setting\n",
    "seed = 0\n",
    "exp_name = 'resnet10_tinyimg_totinyimg'\n",
    "\n",
    "data_kwargs = {'dataset': 'tiny-imagenet',\n",
    "               'data_path': './data/tiny-imagenet-200',\n",
    "               'batch_size': 128,\n",
    "               'num_workers': 32,\n",
    "               'download': False}\n",
    "\n",
    "distributed_kwargs = {'multiprocessing_distributed': False,\n",
    "                      'dist_url': 'tcp://localhost:10001',\n",
    "                      'world_size': 1,\n",
    "                      'rank': 0,\n",
    "                      'dist_backend': 'nccl',\n",
    "                      'gpu': [0]}\n",
    "\n",
    "arch = 'resnet10'\n",
    "arch_kwargs = {'num_classes': 200}\n",
    "arch_kwargs['single_gpu'] = False if len(distributed_kwargs['gpu']) > 1 else True\n",
    "arch_kwargs['num_splits'] = int(data_kwargs['batch_size']/2) if arch_kwargs['single_gpu'] else None\n",
    "\n",
    "train_kwargs = {'pretrained': './results/pretrained/mixco_resnet10.pth.tar',\n",
    "                'print_freq': 10,\n",
    "                'num_epochs': 1,\n",
    "                'schedule': [60, 80],\n",
    "                'cos': True,\n",
    "                'opt_kwargs': {'lr': 3.0, 'momentum': 0.9}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use GPU: 0 for training\n",
      "=> creating model 'resnet10'\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (block_layers): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Block(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Block(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Block(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Block(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn2): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
      ")\n",
      "=> loading checkpoint './results/pretrained/mixco_resnet10.pth.tar'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangmin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.\n",
      "  if __name__ == '__main__':\n",
      "/home/sangmin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded pre-trained model './results/pretrained/mixco_resnet10.pth.tar'\n",
      "Epoch: [1][  0/781]\tTime  3.149 ( 3.149)\tData  1.994 ( 1.994)\tLoss 5.3716e+00 (5.3716e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.78 (  0.78)\n",
      "Epoch: [1][ 10/781]\tTime  0.081 ( 0.378)\tData  0.000 ( 0.198)\tLoss 3.6322e+02 (2.5147e+02)\tAcc@1   1.56 (  0.57)\tAcc@5  10.16 (  3.41)\n",
      "Epoch: [1][ 20/781]\tTime  0.079 ( 0.237)\tData  0.000 ( 0.104)\tLoss 1.2571e+03 (5.0932e+02)\tAcc@1   0.78 (  0.60)\tAcc@5   3.12 (  4.13)\n",
      "Epoch: [1][ 30/781]\tTime  0.098 ( 0.188)\tData  0.020 ( 0.071)\tLoss 9.1866e+02 (6.2624e+02)\tAcc@1   0.78 (  0.63)\tAcc@5   3.91 (  4.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 294, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 492, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/sangmin/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 619, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-297741076bf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfix_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-8a2d2abe68ac>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(func, exp_name, distributed_kwargs, arch, arch_kwargs, train_kwargs, data_kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Simply call main_worker function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngpus_per_node'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b6416bc4212f>\u001b[0m in \u001b[0;36mmain_worker\u001b[0;34m(gpu, ngpus_per_node, exp_name, distributed_kwargs, arch, arch_kwargs, train_kwargs, data_kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'print_freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-41d2cbb74318>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, criterion, epoch, print_freq, gpu)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# measure accuracy and record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0macc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mtop1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtop5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fix_seed(seed)\n",
    "main(main_worker, exp_name, distributed_kwargs, arch, arch_kwargs, train_kwargs, data_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
