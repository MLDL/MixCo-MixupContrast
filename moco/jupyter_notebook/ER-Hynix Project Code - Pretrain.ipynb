{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-Hynix Project Code - Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SK-Hynix 프로젝트에서 진행한 연구의 실험 중, encoder 학습코드를 정리한 Jupyter notebook 파일입니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  1. 'Mixup 기법으로 얻은 representation을 contrastive task에 사용하는 것'이 본 연구의 핵심 아이디어입니다.\n",
    "\n",
    "#### 2. 현재는 [MoCo](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf) 논문을 기반으로 아이디어를 구현했습니다.\n",
    "\n",
    "#### 3. 아쉽게도 multi-gpu 실험은 현 jupyter notebook에서는 불가능합니다. 각 함수들의 기능만 봐주시면 될 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library setting\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os, math, random, time, shutil, builtins, argparse, warnings, json, glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 1] 데이터 불러오기\n",
    "\n",
    "#### CIFAR-10과 CIFAR-100 데이터의 경우, torchvision.datasets library에서 받아옵니다.\n",
    "#### 반면 Tiny-ImageNet 실험의 경우, 현 directory 안에 data/tiny-imagenet-200 이 저장되어 있어야합니다. Tiny-ImageNet-200의 경우 [Tiny-ImageNet](https://tiny-imagenet.herokuapp.com/)에서 다운로드 가능합니다.\n",
    "\n",
    "### <span style=\"color:red\">Hynix 데이터에 적용하기 위해선 다음 두가지의 코드 구현이 필요해보입니다.</span>\n",
    "- **pytorch library의 DataLoader에 데이터를 옮기는 코드**\n",
    "- **데이터에 맞는 augmentation 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10, CIFAR-100 dataset\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "\n",
    "\n",
    "# Tiny-ImageNet dataset\n",
    "EXTENSION = 'JPEG'\n",
    "NUM_IMAGES_PER_CLASS = 500\n",
    "CLASS_LIST_FILE = 'wnids.txt'\n",
    "VAL_ANNOTATION_FILE = 'val_annotations.txt'\n",
    "\n",
    "class TinyImageNet(Dataset):\n",
    "    \"\"\"Tiny ImageNet data set available from `http://cs231n.stanford.edu/tiny-imagenet-200.zip`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    root: string\n",
    "        Root directory including `train`, `test` and `val` subdirectories.\n",
    "    split: string\n",
    "        Indicating which split to return as a data set.\n",
    "        Valid option: [`train`, `test`, `val`]\n",
    "    transform: torchvision.transforms\n",
    "        A (series) of valid transformation(s).\n",
    "    in_memory: bool\n",
    "        Set to True if there is enough memory (about 5G) and want to minimize disk IO overhead.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, in_memory=False, download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.train = train\n",
    "        self.split = 'train' if train else 'val'\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.in_memory = in_memory\n",
    "        self.split_dir = os.path.join(root, self.split)\n",
    "        self.image_paths = sorted(glob.iglob(os.path.join(self.split_dir, '**', '*.%s' % EXTENSION), recursive=True))\n",
    "        self.labels = {}  # fname - label number mapping\n",
    "        self.images = []  # used for in-memory processing\n",
    "\n",
    "        # build class label - number mapping\n",
    "        with open(os.path.join(self.root, CLASS_LIST_FILE), 'r') as fp:\n",
    "            self.label_texts = sorted([text.strip() for text in fp.readlines()])\n",
    "        self.label_text_to_number = {text: i for i, text in enumerate(self.label_texts)}\n",
    "\n",
    "        if self.split == 'train':\n",
    "            for label_text, i in self.label_text_to_number.items():\n",
    "                for cnt in range(NUM_IMAGES_PER_CLASS):\n",
    "                    self.labels['%s_%d.%s' % (label_text, cnt, EXTENSION)] = i\n",
    "        elif self.split == 'val':\n",
    "            with open(os.path.join(self.split_dir, VAL_ANNOTATION_FILE), 'r') as fp:\n",
    "                for line in fp.readlines():\n",
    "                    terms = line.split('\\t')\n",
    "                    file_name, label_text = terms[0], terms[1]\n",
    "                    self.labels[file_name] = self.label_text_to_number[label_text]\n",
    "\n",
    "        # read all images into torch tensor in memory to minimize disk IO overhead\n",
    "        if self.in_memory:\n",
    "            self.images = [self.read_image(path) for path in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.image_paths[index]\n",
    "\n",
    "        if self.in_memory:\n",
    "            img = self.images[index]\n",
    "        else:\n",
    "            img = self.read_image(file_path)\n",
    "\n",
    "        if self.split == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            # file_name = file_path.split('/')[-1]\n",
    "            return img, self.labels[os.path.basename(file_path)]\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = self.split\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "    def read_image(self, path):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        return self.transform(img) if self.transform else img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation 관련 코드입니다.\n",
    "- 다른 augmentation이 적용된 query와 key를 뽑기 위한 **TwoCropsTransform** 코드\n",
    "- MoCo 방법론에서 사용했던 **GaussianBlur**(SimCLR paper에서 사용한 것을 차용) augmentation 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "class TwoCropsTransform:\n",
    "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
    "\n",
    "    def __init__(self, base_transform):\n",
    "        self.base_transform = base_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        q = self.base_transform(x)\n",
    "        k = self.base_transform(x)\n",
    "        return [q, k]\n",
    "\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[.1, 2.]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder 학습을 위해 선택한 데이터셋을 pytorch library의 DataLoader로 옮기는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "DATASETS = {'cifar10': CIFAR10, 'cifar100': CIFAR100, 'tiny-imagenet': TinyImageNet}\n",
    "MEAN = {'cifar10': [0.4914, 0.4822, 0.4465], 'cifar100': [0.5071, 0.4867, 0.4408], 'tiny-imagenet': [0.485, 0.456, 0.406]}\n",
    "STD = {'cifar10': [0.2023, 0.1994, 0.2010], 'cifar100':[0.2675, 0.2565, 0.2761], 'tiny-imagenet': [0.229, 0.224, 0.225]}\n",
    "\n",
    "def data_loader(dataset, data_path, batch_size, num_workers, download=False, distributed=True, aug_plus=True):\n",
    "    normalize = transforms.Normalize(MEAN[dataset], STD[dataset])\n",
    "\n",
    "    if aug_plus:\n",
    "        # MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709\n",
    "        augmentation = [\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ]\n",
    "    else:\n",
    "        # MoCo v1's aug: the same as InstDisc https://arxiv.org/abs/1805.01978\n",
    "        augmentation = [\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ]\n",
    "\n",
    "    augmentation.insert(0, transforms.RandomResizedCrop(224, scale=(0.2, 1.)))\n",
    "\n",
    "    train_transform = TwoCropsTransform(transforms.Compose(augmentation))\n",
    "    train_dataset = DATASETS[dataset](data_path, train=True, download=download, transform=train_transform)\n",
    "\n",
    "    # for distributed learning\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if distributed else None\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),\n",
    "        num_workers=num_workers, pin_memory=True, sampler=train_sampler, drop_last=True)\n",
    "    \n",
    "    return train_loader, train_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 2] 모델 불러오기\n",
    "\n",
    "#### MoCo 방법론을 기반으로 코드를 구현했고, MoCo나 MixCo에 관한 argument 설정을 통해 두 방법론을 사용하실 수 있습니다.\n",
    "#### Encoder의 기본 model로는 ResNet을 사용하였습니다.\n",
    "\n",
    "\n",
    "### <span style=\"color:red\">다양한 모델 적용을 위해선, pytorch에 구현된 모델 코드들을 가져와 사용하시면 될 것 같습니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet code\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    __constants__ = ['downsample']\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, \n",
    "                 base_width=64, dilation=1, norm_layer=None, num_splits=64, expansion=1, block_type='basic'):\n",
    "        super(Block, self).__init__()\n",
    "        if block_type not in ['basic', 'bottleneck']:\n",
    "            raise ValueError('Block_Type only supports basic and bottleneck')\n",
    "        self.block_type = block_type\n",
    "        self.expansion = expansion\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "            self.split = False\n",
    "        else:\n",
    "            self.split = True\n",
    "            \n",
    "        if block_type == 'basic':\n",
    "            if groups != 1 or base_width != 64:\n",
    "                raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "            if dilation > 1:\n",
    "                raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        \n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both conv3*3 with stride and self.downsample layers downsample the input when stride != 1\n",
    "        if block_type == 'basic':\n",
    "            self.conv1 = conv3x3(inplanes, width, stride)\n",
    "            self.conv2 = conv3x3(width, width)\n",
    "            \n",
    "        if block_type == 'bottleneck':\n",
    "            self.conv1 = conv1x1(inplanes, width)\n",
    "            self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "            self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "            self.bn3 = norm_layer(planes * self.expansion) if not self.split else norm_layer(planes * self.expansion, num_splits)\n",
    "            \n",
    "        self.bn1 = norm_layer(width) if not self.split else norm_layer(width, num_splits)\n",
    "        self.bn2 = norm_layer(width) if not self.split else norm_layer(width, num_splits)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.block_type == 'bottleneck':\n",
    "            out = self.relu(out)\n",
    "            \n",
    "            out = self.conv3(out)\n",
    "            out = self.bn3(out)\n",
    "            \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    BasicBlock_arch = ['resnet10', 'resnet18', 'resnet34']\n",
    "    Bottleneck_arch = ['resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', \n",
    "                      'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "    def __init__(self, arch, repeats, num_classes=100, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, norm_layer=None, num_splits=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.split = False if norm_layer is None else True\n",
    "        self._norm_layer = nn.BatchNorm2d if norm_layer is None else norm_layer\n",
    "        self.num_splits = num_splits\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        if arch in self.BasicBlock_arch:\n",
    "            self.expansion = 1\n",
    "            self.block_type = 'basic'\n",
    "        elif arch in self.Bottleneck_arch:\n",
    "            self.expansion = 4\n",
    "            self.block_type = 'bottleneck'\n",
    "        else:\n",
    "            raise NotImplementedError('%s arch is not supported in ResNet' % arch)\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                                   bias=False)\n",
    "        self.bn1 = self._norm_layer(self.inplanes) if not self.split else self._norm_layer(self.inplanes, self.num_splits)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)     \n",
    "            \n",
    "        planes = [64, 128, 256, 512]\n",
    "        # self.planes attributes is needed to match with EP_module channels\n",
    "        self.planes = [p * self.expansion for p in planes]\n",
    "        strides = [1, 2, 2, 2]\n",
    "        self.block_layers = self._make_layer(planes, repeats, strides)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(planes[-1] * self.expansion, num_classes)\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Block):\n",
    "                    if self.block_type == 'basic':\n",
    "                        nn.init.constant_(m.bn2.weight, 0)\n",
    "                    elif self.block_type == 'bottleneck':\n",
    "                        nn.init.constant_(m.bn3.weight, 0)\n",
    "                        \n",
    "    def _make_layer(self, planes, repeats, strides):\n",
    "        assert len(planes) == len(repeats) == len(strides) == 4, 'Number of Block should be 4'\n",
    "        \n",
    "        block_layers = []\n",
    "        norm_layer = self._norm_layer\n",
    "        for i in  range(4):\n",
    "            plane = planes[i]\n",
    "            repeat = repeats[i]\n",
    "            stride = strides[i]\n",
    "            \n",
    "            downsample = None\n",
    "            if stride != 1 or self.inplanes != plane * self.expansion:\n",
    "                if not self.split:\n",
    "                    downsample = nn.Sequential(\n",
    "                        conv1x1(self.inplanes, plane * self.expansion, stride),\n",
    "                        norm_layer(plane * self.expansion),\n",
    "                    )\n",
    "                else:\n",
    "                    downsample = nn.Sequential(\n",
    "                        conv1x1(self.inplanes, plane * self.expansion, stride),\n",
    "                        norm_layer(plane * self.expansion, self.num_splits),\n",
    "                    )\n",
    "\n",
    "            layers = []\n",
    "            layers.append(Block(self.inplanes, plane, stride, downsample, self.groups,\n",
    "                                self.base_width, self.dilation, norm_layer, self.num_splits,\n",
    "                                self.expansion, self.block_type))\n",
    "            self.inplanes = plane * self.expansion\n",
    "            for _ in range(1, repeat):\n",
    "                layers.append(Block(self.inplanes, plane, groups=self.groups,\n",
    "                                    base_width=self.base_width, dilation=self.dilation,\n",
    "                                    norm_layer=norm_layer, num_splits=self.num_splits,\n",
    "                                    expansion=self.expansion, block_type=self.block_type))\n",
    "            block_layers.append(nn.Sequential(*layers))\n",
    "        \n",
    "        return nn.Sequential(*block_layers)\n",
    "    \n",
    "    def conv_stem(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def pool_linear(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.block_layers(x)\n",
    "        x = self.pool_linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def _resnet(arch, repeats, **kwargs):\n",
    "    model = ResNet(arch, repeats, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet10(**kwargs):\n",
    "    return _resnet('resnet10', [1, 1, 1, 1], **kwargs)    \n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return _resnet('resnet18', [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return _resnet('resnet34', [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return _resnet('resnet50', [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet101( **kwargs):\n",
    "    return _resnet('resnet101', [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    return _resnet('resnet152', [3, 8, 36, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet50_32x4d(**kwargs):\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnet50_32x4d', [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet101_32x8d(**kwargs):\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnet101_32x8d', [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(**kwargs):\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(**kwargs):\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', [3, 4, 23, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have other architecture, type into ARCHITECTURE dict\n",
    "ARCHITECTURE = {'resnet10': resnet10, 'resnet18': resnet18, 'resnet34': resnet34, 'resnet50': resnet50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MoCo 논문에서 multi-gpu 상황에서 성능 향상을 위해 Batch Shuffling이란 technique을 사용했습니다.\n",
    "#### 하지만 Single-gpu에서는 shuffling 적용이 불가해, encoder에서 사용하는 BN을 다음의 SplitBatchNorm으로 바꿀 것을 권장하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SplitBatchNorm: Same effect with Batch Shuffling in MoCo\n",
    "class SplitBatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, num_splits, **kw):\n",
    "        super().__init__(num_features, **kw)\n",
    "        self.num_splits = num_splits\n",
    "        \n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            running_mean_split = self.running_mean.repeat(self.num_splits)\n",
    "            running_var_split = self.running_var.repeat(self.num_splits)\n",
    "            outcome = nn.functional.batch_norm(\n",
    "                input.view(-1, C * self.num_splits, H, W), running_mean_split, running_var_split, \n",
    "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "                True, self.momentum, self.eps).view(N, C, H, W)\n",
    "            self.running_mean.data.copy_(running_mean_split.view(self.num_splits, C).mean(dim=0))\n",
    "            self.running_var.data.copy_(running_var_split.view(self.num_splits, C).mean(dim=0))\n",
    "            return outcome\n",
    "        else:\n",
    "            return nn.functional.batch_norm(\n",
    "                input, self.running_mean, self.running_var, \n",
    "                self.weight, self.bias, False, self.momentum, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder class의 mixco argument를 통해, MoCo 방법론과 MixCo 방법론을 선택할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a MoCo model with: a query encoder, a key encoder, and a queue\n",
    "    https://arxiv.org/abs/1911.05722\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, algo, dim=128, num_splits=64, K=65536, m=0.999, T=0.2, mix_T=0.05, mlp=False, single_gpu=False):\n",
    "        \"\"\"\n",
    "        dim: feature dimension (default: 128)\n",
    "        K: queue size; number of negative keys (default: 65536)\n",
    "        m: moco momentum of updating key encoder (default: 0.999)\n",
    "        T: softmax temperature (default: 0.07)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.algo = algo\n",
    "        self.single_gpu = single_gpu\n",
    "        \n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        self.mix_T = mix_T\n",
    "\n",
    "        # create the encoders\n",
    "        # num_classes is the output fc dimension\n",
    "        \n",
    "        norm_layer = SplitBatchNorm if single_gpu else None\n",
    "        self.encoder_q = base_encoder(num_classes=dim, norm_layer=norm_layer, num_splits=num_splits)\n",
    "        self.encoder_k = base_encoder(num_classes=dim, norm_layer=norm_layer, num_splits=num_splits)\n",
    "\n",
    "        if mlp:  # hack: brute-force replacement\n",
    "            dim_mlp = self.encoder_q.fc.weight.shape[1]\n",
    "            self.encoder_q.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.encoder_q.fc)\n",
    "            self.encoder_k.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.encoder_k.fc)\n",
    "\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient\n",
    "\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def concat_all_gather(self, tensor):\n",
    "        \"\"\"\n",
    "        Performs all_gather operation on the provided tensors.\n",
    "        *** Warning ***: torch.distributed.all_gather has no gradient.\n",
    "        \"\"\"\n",
    "        tensors_gather = [torch.ones_like(tensor)\n",
    "            for _ in range(torch.distributed.get_world_size())]\n",
    "        torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n",
    "\n",
    "        output = torch.cat(tensors_gather, dim=0)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        # gather keys before updating queue\n",
    "        if not self.single_gpu:\n",
    "            keys = self.concat_all_gather(keys)\n",
    "\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _batch_shuffle_ddp(self, x):\n",
    "        \"\"\"\n",
    "        Batch shuffle, for making use of BatchNorm.\n",
    "        *** Only support DistributedDataParallel (DDP) model. ***\n",
    "        \"\"\"\n",
    "        # gather from all gpus\n",
    "        batch_size_this = x.shape[0]\n",
    "        x_gather = self.concat_all_gather(x)\n",
    "        batch_size_all = x_gather.shape[0]\n",
    "\n",
    "        num_gpus = batch_size_all // batch_size_this\n",
    "\n",
    "        # random shuffle index\n",
    "        idx_shuffle = torch.randperm(batch_size_all).cuda()\n",
    "\n",
    "        # broadcast to all gpus\n",
    "        torch.distributed.broadcast(idx_shuffle, src=0)\n",
    "\n",
    "        # index for restoring\n",
    "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
    "\n",
    "        # shuffled index for this gpu\n",
    "        gpu_idx = torch.distributed.get_rank()\n",
    "        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]\n",
    "\n",
    "        return x_gather[idx_this], idx_unshuffle\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _batch_unshuffle_ddp(self, x, idx_unshuffle):\n",
    "        \"\"\"\n",
    "        Undo batch shuffle.\n",
    "        *** Only support DistributedDataParallel (DDP) model. ***\n",
    "        \"\"\"\n",
    "        # gather from all gpus\n",
    "        batch_size_this = x.shape[0]\n",
    "        x_gather = self.concat_all_gather(x)\n",
    "        batch_size_all = x_gather.shape[0]\n",
    "\n",
    "        num_gpus = batch_size_all // batch_size_this\n",
    "\n",
    "        # restored index for this gpu\n",
    "        gpu_idx = torch.distributed.get_rank()\n",
    "        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]\n",
    "\n",
    "        return x_gather[idx_this]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def img_mixer(self, im_q):\n",
    "        B = im_q.size(0)\n",
    "        assert B % 2 == 0\n",
    "        sid = int(B/2)\n",
    "        im_q1, im_q2 = im_q[:sid], im_q[sid:]\n",
    "        \n",
    "        # each image get different lambda\n",
    "        lam = torch.from_numpy(np.random.uniform(0, 1, size=(sid,1,1,1))).float().to(im_q.device)\n",
    "        imgs_mix = lam * im_q1 + (1-lam) * im_q2\n",
    "        lbls_mix = torch.cat((torch.diag(lam.squeeze()), torch.diag((1-lam).squeeze())), dim=1)\n",
    "        \n",
    "        return imgs_mix, lbls_mix\n",
    "    \n",
    "    def forward(self, im_q, im_k):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            im_q: a batch of query images\n",
    "            im_k: a batch of key images\n",
    "        Output:\n",
    "            logits, targets\n",
    "        \"\"\"\n",
    "\n",
    "        if self.algo == 'moco':\n",
    "            q = self.encoder_q(im_q) # queries: NxC\n",
    "            q = nn.functional.normalize(q, dim=1)\n",
    "            \n",
    "        elif self.algo == 'mixco':\n",
    "            imgs_mix, lbls_mix = self.img_mixer(im_q)\n",
    "            # compute query features\n",
    "            q = self.encoder_q(torch.cat((im_q, imgs_mix))) # queries: NxC\n",
    "            q = nn.functional.normalize(q, dim=1)\n",
    "\n",
    "            q_mix = q[im_q.size(0):]\n",
    "            q = q[:im_q.size(0)]\n",
    "\n",
    "        # compute key features\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            self._momentum_update_key_encoder()  # update the key encoder\n",
    "\n",
    "            # shuffle for making use of BN\n",
    "            if not self.single_gpu:\n",
    "                im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)\n",
    "\n",
    "            k = self.encoder_k(im_k)  # keys: NxC\n",
    "            k = nn.functional.normalize(k, dim=1)\n",
    "\n",
    "            # undo shuffle\n",
    "            if not self.single_gpu:\n",
    "                k = self._batch_unshuffle_ddp(k, idx_unshuffle)\n",
    "            \n",
    "\n",
    "        # compute logits\n",
    "        # Einstein sum is more intuitive\n",
    "        # positive logits: Nx1\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        # negative logits: NxK\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "\n",
    "        # logits: Nx(1+K)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "\n",
    "        # labels: positive key indicators\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n",
    "        \n",
    "        if self.algo == 'moco':\n",
    "            # apply temperature\n",
    "            logits /= self.T\n",
    "\n",
    "            # dequeue and enqueue\n",
    "            self._dequeue_and_enqueue(k)\n",
    "\n",
    "            return logits, labels\n",
    "            \n",
    "        elif self.algo == 'mixco':\n",
    "            # mixed logits: N/2 x N\n",
    "            logits_mix_pos = torch.mm(q_mix, k.transpose(0, 1)) \n",
    "            # mixed negative logits: N/2 x K\n",
    "            logits_mix_neg = torch.mm(q_mix, self.queue.clone().detach())\n",
    "            logits_mix = torch.cat([logits_mix_pos, logits_mix_neg], dim=1) # N/2 x (N+K)\n",
    "            lbls_mix = torch.cat([lbls_mix, torch.zeros_like(logits_mix_neg)], dim=1)\n",
    "\n",
    "            # apply temperature\n",
    "            logits /= self.T\n",
    "            logits_mix /= self.mix_T\n",
    "\n",
    "            # dequeue and enqueue\n",
    "            self._dequeue_and_enqueue(k)\n",
    "\n",
    "            return logits, labels, logits_mix, lbls_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 3] 손실함수 정의하기\n",
    "\n",
    "#### Section 2의 Encoder가 반환해준 logit과 label에 대해서, contrastive 손실 함수를 통해 모델을 학습할 것입니다.\n",
    "#### 기존의 MoCo에서 사용하는 contrastive loss는 다음과 같습니다.\n",
    "\n",
    "\\begin{equation*}\n",
    "L_{𝑀𝑜𝐶𝑜}= -\\sum_{i=1}^n log(\\frac{exp(\\frac{v_i \\cdot v_{i}^{'}}{\\tau})}{\\sum_{j=0}^{r} exp(\\frac{v_i \\cdot v_{j}^{'}}{\\tau})})\n",
    "\\end{equation*}\n",
    "\n",
    "#### MixCo 방법론의 경우, 다음의 손실함수를 통해 학습합니다.\n",
    "\n",
    "\\begin{equation*}\n",
    "L_{MixCo}= L_{MoCo} + \\gamma * -(\\sum_{i=1}^n \\lambda_{i} * log(\\frac{exp(\\frac{v_{i}^{mix_{ij}} \\cdot v_{i}^{'}}{\\tau_{mix}})}{\\sum_{k=0}^{r} exp(\\frac{v_{i}^{mix_{ij}} \\cdot v_{k}^{'}}{\\tau_{mix}})}) + (1 - \\lambda_{i}) * log(\\frac{exp(\\frac{v_{i}^{mix_{ij}} \\cdot v_{j}^{'}}{\\tau_{mix}})}{\\sum_{k=0}^{r} exp(\\frac{v_{i}^{mix_{ij}} \\cdot v_{k}^{'}}{\\tau_{mix}})}))\n",
    "\\end{equation*}\n",
    "\n",
    "#### i번째 이미지와 j번째 이미지의 mixup으로 얻은 representation에 대한 contrastive 손실함수를 기존의 MoCo 손실함수에 더해주는 방법으로 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftCrossEntropy, self).__init__()\n",
    "        \n",
    "    def forward(self, logits, target):\n",
    "        probs = F.softmax(logits, 1) \n",
    "        nll_loss = (- target * torch.log(probs)).sum(1).mean()\n",
    "\n",
    "        return nll_loss\n",
    "\n",
    "    \n",
    "class MixcoLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super(MixcoLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.soft_loss = SoftCrossEntropy()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        if not self.gamma:\n",
    "            logits, labels = outputs\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        else:\n",
    "            logits, labels, logits_mix, lbls_mix = outputs\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            loss += self.gamma * self.soft_loss(logits_mix, lbls_mix)\n",
    "        \n",
    "        return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 4] 학습 함수 구현하기\n",
    "\n",
    "#### 먼저 학습 과정에서 사용될 seed 고정, learning_rate 조절, checkpoint 저장 등의 utils 함수들을 구현하였습니다. \n",
    "#### 그 다음 encoder의 한 epoch 단위의 학습 함수를 구현하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def fix_seed(seed):\n",
    "    # fix seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "    warnings.warn('You have chosen to seed training. '\n",
    "                  'This will turn on the CUDNN deterministic setting, '\n",
    "                  'which can slow down your training considerably! '\n",
    "                  'You may see unexpected behavior when restarting '\n",
    "                  'from checkpoints.')\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='test'):\n",
    "    filename = os.path.join('./results/pretrained', filename)\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, cos, num_epochs, schedule):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    lr = lr\n",
    "    if cos:  # cosine lr schedule\n",
    "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / num_epochs))\n",
    "    else:  # stepwise lr schedule\n",
    "        for milestone in schedule:\n",
    "            lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "    \n",
    "    \n",
    "def update_json(exp_name, part='pretrain', acc=[0,0], path='./results/results.json'):\n",
    "    acc = [round(acc[0], 3), round(acc[1], 3)]\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        result_dict = json.load(f)\n",
    "    \n",
    "        if exp_name not in result_dict.keys():\n",
    "            result_dict[exp_name] = dict()\n",
    "\n",
    "        result_dict[exp_name][part] = acc\n",
    "    \n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(result_dict, f)\n",
    "        \n",
    "    print('results updated to %s' % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, criterion, epoch, print_freq, gpu):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if gpu is not None:\n",
    "            images[0] = images[0].cuda(gpu, non_blocking=True)\n",
    "            images[1] = images[1].cuda(gpu, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(im_q=images[0], im_k=images[1])\n",
    "        loss = criterion(outputs)\n",
    "\n",
    "        # acc1/acc5 are (K+1)-way contrast classifier accuracy\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(outputs[0], outputs[1], topk=(1, 5))\n",
    "        losses.update(loss.item(), images[0].size(0))\n",
    "        top1.update(acc1[0], images[0].size(0))\n",
    "        top5.update(acc5[0], images[0].size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "            \n",
    "    return top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 5] 분산학습 환경 설정하기\n",
    "\n",
    "#### 이전 Section들에서 정의했던 함수와 class들을 이용해, 전체적인 main_worker 함수를 구현하였습니다. \n",
    "#### pytorch에서 제공하는 분산환경 구축 코드를 참조하여 'main' 함수를 구현였습니다. 'func' argument로는 'main_worker' function을 넣으면 됩니다. \n",
    "#### 참고로, 분산환경을 사용할 때는 데이터를 불러오는 과정에서 DistributedSampler를 사용해야합니다. (Section 1 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(gpu, ngpus_per_node, exp_name, distributed_kwargs, algo, arch, arch_kwargs, train_kwargs, data_kwargs):\n",
    "    gpu = gpu\n",
    "\n",
    "    # suppress printing if not master\n",
    "    if distributed_kwargs['multiprocessing_distributed'] and gpu != 0:\n",
    "        def print_pass(*args):\n",
    "            pass\n",
    "        builtins.print = print_pass\n",
    "\n",
    "    if gpu is not None:\n",
    "        print(gpu)\n",
    "        print(\"Use GPU: {} for training\".format(gpu))\n",
    "\n",
    "    if distributed_kwargs['distributed']:\n",
    "        if distributed_kwargs['dist_url'] == \"env://\" and distributed_kwargs['rank'] == -1:\n",
    "            rank = int(os.environ[\"RANK\"])\n",
    "        if distributed_kwargs['multiprocessing_distributed']:\n",
    "            # For multiprocessing distributed training, rank needs to be the\n",
    "            # global rank among all the processes\n",
    "            rank = rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend=distributed_kwargs['dist_backend'], \n",
    "                                init_method=distributed_kwargs['dist_url'],\n",
    "                                world_size=distributed_kwargs['world_size'],\n",
    "                                rank=rank)\n",
    "    # create model\n",
    "    print(\"=> creating model '{}'\".format(arch))\n",
    "    \n",
    "    model = Encoder(ARCHITECTURE[arch], algo, **arch_kwargs)\n",
    "    print(model)\n",
    "\n",
    "    if distributed_kwargs['distributed']:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if gpu is not None:\n",
    "            torch.cuda.set_device(gpu)\n",
    "            model.cuda(gpu)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            data_kwargs['batch_size'] = int(data_kwargs['batch_size'] / ngpus_per_node)\n",
    "            data_kwargs['num_workers'] = int((data_kwargs['num_workers'] + ngpus_per_node - 1) / ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif gpu is not None:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        model = model.cuda(gpu)\n",
    "        \n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), **train_kwargs['opt_kwargs'])\n",
    "    criterion = MixcoLoss(train_kwargs['gamma']).cuda(gpu)\n",
    "\n",
    "    # get train_loader\n",
    "    train_loader, train_sampler = data_loader(**data_kwargs)\n",
    "    \n",
    "    for epoch in range(train_kwargs['num_epochs']):\n",
    "        if distributed_kwargs['distributed']:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        adjust_learning_rate(optimizer, epoch, train_kwargs['opt_kwargs']['lr'], train_kwargs['cos'], train_kwargs['num_epochs'], train_kwargs['schedule'])\n",
    "\n",
    "        # train for one epoch\n",
    "        acc1, acc5 = train(train_loader, model, optimizer, criterion, epoch+1, train_kwargs['print_freq'], gpu)\n",
    "\n",
    "    # always saves at the end of training    \n",
    "    else:\n",
    "        if not distributed_kwargs['multiprocessing_distributed'] \\\n",
    "        or (distributed_kwargs['multiprocessing_distributed'] and rank % ngpus_per_node == 0):\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch+1,\n",
    "                'arch': arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best=False, filename='{}.pth.tar'.format(exp_name))\n",
    "            \n",
    "            update_json(exp_name, 'pretrain', [acc1.item(), acc5.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(func, exp_name, distributed_kwargs, algo, arch, arch_kwargs, train_kwargs, data_kwargs):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([str(gpu) for gpu in distributed_kwargs['gpu']])\n",
    "    if len(distributed_kwargs['gpu']) > 1:\n",
    "        distributed_kwargs['gpu'] = None\n",
    "    else:\n",
    "        distributed_kwargs['gpu'] = [0]\n",
    "\n",
    "    if distributed_kwargs['gpu'] is not None:\n",
    "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                      'disable data parallelism.')\n",
    "\n",
    "    if distributed_kwargs['dist_url'] == \"env://\" and distributed_kwargs['world_size'] == -1:\n",
    "        distributed_kwargs['world_size'] = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    distributed_kwargs['distributed'] = distributed_kwargs['world_size'] > 1 or distributed_kwargs['multiprocessing_distributed']\n",
    "    distributed_kwargs['ngpus_per_node'] = torch.cuda.device_count()\n",
    "    \n",
    "    data_kwargs['distributed'] = distributed_kwargs['distributed']\n",
    "    \n",
    "    if distributed_kwargs['multiprocessing_distributed']:\n",
    "        # Since we have ngpus_per_node processes per node, the total world_size\n",
    "        # needs to be adjusted accordingly\n",
    "        distributed_kwargs['world_size'] *= distributed_kwargs['ngpus_per_node']\n",
    "        # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
    "        # main_worker process function\n",
    "        mp.spawn(func, nprocs=distributed_kwargs['ngpus_per_node'], args=(distributed_kwargs['ngpus_per_node'], exp_name, distributed_kwargs, algo, arch, arch_kwargs, train_kwargs, data_kwargs))\n",
    "    else:\n",
    "        # Simply call main_worker function\n",
    "        func(distributed_kwargs['gpu'][0], distributed_kwargs['ngpus_per_node'], exp_name, distributed_kwargs, algo, arch, arch_kwargs, train_kwargs, data_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section 6] Encoder 학습하기\n",
    "\n",
    "#### ResNet encoder 모델을 라벨이 없는 Tiny-ImageNet 데이터를 이용해 학습해볼 것입니다.\n",
    "### <span style=\"color:red\">원하는 실험 셋팅에 필요한 argument를 정의하시면 됩니다.</span>\n",
    "- 아래 세팅은 single-gpu 상황에서, mixco 방법론의 실험입니다.\n",
    "- **moco** 방법론으로 실험하려면, **train_kwargs['gamma'] = 0.0** 으로 설정하시면 됩니다.\n",
    "- **multiple-gpu** 상황에서의 실험을 원하시면, **distributed_kwargs['multiprocessing_distributed'] = True** 와 **distributed_kwargs['gpu'] = [gpu_number1, gpu_number2, ...]** 로 설정하시면 됩니다.\n",
    "- <span style=\"color:red\">하지만, jupyter notebook에서는 multi-gpu를 위한 torch.multiprocessing.spawn을 사용할 수 없어 여기서는 불가능합니다.<span style=\"color:red\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories\n",
    "!mkdir -p './results/pretrained'\n",
    "\n",
    "# setting\n",
    "seed = 0\n",
    "exp_name = 'mixco_resnet10'\n",
    "\n",
    "data_kwargs = {'dataset': 'tiny-imagenet',\n",
    "               'data_path': './data/tiny-imagenet-200',\n",
    "               'aug_plus': True,\n",
    "               'batch_size': 128,\n",
    "               'num_workers': 32,\n",
    "               'download': False}\n",
    "\n",
    "distributed_kwargs = {'multiprocessing_distributed': False,\n",
    "                      'dist_url': 'tcp://localhost:13311',\n",
    "                      'world_size': 1,\n",
    "                      'rank': 0,\n",
    "                      'dist_backend': 'nccl',\n",
    "                      'gpu': [0]}\n",
    "\n",
    "algo = 'mixco'\n",
    "arch = 'resnet10'\n",
    "arch_kwargs = {'dim': 128,\n",
    "               'K': 65536,\n",
    "               'm': 0.999,\n",
    "               'T': 0.2,\n",
    "               'mix_T': 0.05,\n",
    "               'mlp': True}\n",
    "arch_kwargs['single_gpu'] = False if len(distributed_kwargs['gpu']) > 1 else True\n",
    "arch_kwargs['num_splits'] = int(data_kwargs['batch_size']/2) if arch_kwargs['single_gpu'] else None\n",
    "\n",
    "train_kwargs = {'print_freq': 10,\n",
    "                'gamma': 1.0,\n",
    "                'num_epochs': 100,\n",
    "                'schedule': [60, 80],\n",
    "                'cos': True,\n",
    "                'opt_kwargs': {'lr': 0.015, 'momentum': 0.9, 'weight_decay': 1e-4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use GPU: 0 for training\n",
      "=> creating model 'resnet10'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangmin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.\n",
      "  if __name__ == '__main__':\n",
      "/home/sangmin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (encoder_q): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (block_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder_k): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (block_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): SplitBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SplitBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SplitBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (bn2): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SplitBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch: [1][  0/781]\tTime 12.686 (12.686)\tData  6.991 ( 6.991)\tLoss 1.1318e+01 (1.1318e+01)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-efdd40e20aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfix_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-388b6edac0b0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(func, exp_name, distributed_kwargs, algo, arch, arch_kwargs, train_kwargs, data_kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Simply call main_worker function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngpus_per_node'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-dee3a06f01b1>\u001b[0m in \u001b[0;36mmain_worker\u001b[0;34m(gpu, ngpus_per_node, exp_name, distributed_kwargs, algo, arch, arch_kwargs, train_kwargs, data_kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0macc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'print_freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# always saves at the end of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-67b95c693ea2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, criterion, epoch, print_freq, gpu)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ba3eb8ce1c3a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im_q, im_k)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# labels: positive key indicators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'moco'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fix_seed(seed)\n",
    "main(main_worker, exp_name, distributed_kwargs, algo, arch, arch_kwargs, train_kwargs, data_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
