{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(indexes, N=None, ignore_index=None):\n",
    "    \"\"\"\n",
    "    Creates a one-representation of indexes with N possible entries\n",
    "    if N is not specified, it will suit the maximum index appearing.\n",
    "    indexes is a long-tensor of indexes\n",
    "    ignore_index will be zero in onehot representation\n",
    "    \"\"\"\n",
    "    if N is None:\n",
    "        N = indexes.max() + 1\n",
    "    sz = list(indexes.size())\n",
    "    output = indexes.new().byte().resize_(*sz, N).zero_()\n",
    "    output.scatter_(-1, indexes.unsqueeze(-1), 1)\n",
    "    if ignore_index is not None and ignore_index >= 0:\n",
    "        output.masked_fill_(indexes.eq(ignore_index).unsqueeze(-1), 0)\n",
    "    return output\n",
    "\n",
    "def _is_long(x):\n",
    "    if hasattr(x, 'data'):\n",
    "        x = x.data\n",
    "    return isinstance(x, torch.LongTensor) or isinstance(x, torch.cuda.LongTensor)\n",
    "\n",
    "\n",
    "def cross_entropy(inputs, target, weight=None, ignore_index=-100, reduction='mean',\n",
    "                  smooth_eps=None, smooth_dist=None, from_logits=True):\n",
    "    \"\"\"cross entropy loss, with support for target distributions and label smoothing https://arxiv.org/abs/1512.00567\"\"\"\n",
    "    smooth_eps = smooth_eps or 0\n",
    "\n",
    "    # ordinary log-liklihood - use cross_entropy from nn\n",
    "    if _is_long(target) and smooth_eps == 0:\n",
    "        if from_logits:\n",
    "            return F.cross_entropy(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n",
    "        else:\n",
    "            return F.nll_loss(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n",
    "\n",
    "    if from_logits:\n",
    "        # log-softmax of inputs\n",
    "        lsm = F.log_softmax(inputs, dim=-1)\n",
    "    else:\n",
    "        lsm = inputs\n",
    "\n",
    "    masked_indices = None\n",
    "    num_classes = inputs.size(-1)\n",
    "\n",
    "    if _is_long(target) and ignore_index >= 0:\n",
    "        masked_indices = target.eq(ignore_index)\n",
    "\n",
    "    if smooth_eps > 0 and smooth_dist is not None:\n",
    "        if _is_long(target):\n",
    "            target = onehot(target, num_classes).type_as(inputs)\n",
    "        if smooth_dist.dim() < target.dim():\n",
    "            smooth_dist = smooth_dist.unsqueeze(0)\n",
    "        target.lerp_(smooth_dist, smooth_eps)\n",
    "\n",
    "    if weight is not None:\n",
    "        lsm = lsm * weight.unsqueeze(0)\n",
    "\n",
    "    if _is_long(target):\n",
    "        eps_sum = smooth_eps / num_classes\n",
    "        eps_nll = 1. - eps_sum - smooth_eps\n",
    "        likelihood = lsm.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = -(eps_nll * likelihood + eps_sum * lsm.sum(-1))\n",
    "    else:\n",
    "        loss = -(target * lsm).sum(-1)\n",
    "\n",
    "    if masked_indices is not None:\n",
    "        loss.masked_fill_(masked_indices, 0)\n",
    "\n",
    "    if reduction == 'sum':\n",
    "        loss = loss.sum()\n",
    "    elif reduction == 'mean':\n",
    "        if masked_indices is None:\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = loss.sum() / float(loss.size(0) - masked_indices.sum())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.random.beta(1,1, size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.randperm(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = torch.randn(256, 3, 32, 32)\n",
    "x_b = torch.randn(256, 3, 32, 32)\n",
    "\n",
    "rep_a = torch.randn(256, 128) # B * dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mixer(x_aug1, x_aug2, alpha=1.0, eps=0.0):\n",
    "    device = x_aug1.cuda()\n",
    "    \n",
    "    # batch size\n",
    "    b = x_aug1.shape[0]\n",
    "    \n",
    "    idx1 = torch.Tensor(range(256)).cuda()\n",
    "    idx2 = torch.randperm(b).cuda()\n",
    "    \n",
    "    # mixup process\n",
    "    lam = torch.Tensor(np.random.beta(alpha, alpha, size=b)).to(device)\n",
    "    lam = lam.reshape(b,1,1,1)\n",
    "    x_aug2 = x_aug2[index]    # shuffle samples\n",
    "    x_mix = lam*x_aug1 + (1-lam)*x_aug2\n",
    "    \n",
    "    lam = lam.reshape(b,1)\n",
    "    \n",
    "    target1 = onehot(idx1.long(), N=b)\n",
    "    target1 = eps*(torch.ones(b)/b) + (1-eps) * target1\n",
    "    target2 = onehot(idx2.long(), N=b)\n",
    "    target2 = eps*(torch.ones(b)/b) + (1-eps) * target2\n",
    "    \n",
    "    ins_label = lam*target1 + (1-lam)*target2\n",
    "    \n",
    "    return x_mix, lam, ins_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(256,128)\n",
    "b = torch.randn(256,128)\n",
    "\n",
    "target = torch.randn(256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mix, lam, ins_label = data_mixer(x_a, x_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, dim=1)[1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-11.0752)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
